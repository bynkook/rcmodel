# ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡ - ë…¼ë¬¸ êµ¬í˜„ ì½”ë“œ
# íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ì ìš©

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.base import BaseEstimator, ClassifierMixin
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸
# ============================================================================

def load_and_explore_data():
    """íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë¡œë“œ ë° ê¸°ë³¸ íƒìƒ‰"""
    # íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë¡œë“œ
    titanic = sns.load_dataset('titanic')
    
    print("=== íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ ===")
    print(f"ë°ì´í„° í˜•íƒœ: {titanic.shape}")
    print(f"\nì»¬ëŸ¼ ì •ë³´:")
    print(titanic.info())
    
    print(f"\nê²°ì¸¡ê°’ í™•ì¸:")
    print(titanic.isnull().sum())
    
    print(f"\nìƒì¡´ë¥  ë¶„í¬:")
    print(titanic['survived'].value_counts(normalize=True))
    
    return titanic

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ (ë…¼ë¬¸ ë°©ì‹ ì ìš©)
# ============================================================================

def preprocess_titanic_data(df):
    """
    ë…¼ë¬¸ì˜ ì „ì²˜ë¦¬ ë°©ì‹ì„ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì— ì ìš©
    - ê²°ì¸¡ê°’ì„ 'Missing' ë¬¸ìì—´ë¡œ ëŒ€ì²´
    - ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
    """
    data = df.copy()
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ì¼ë°˜ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ê²°ì¸¡ê°’ ì²˜ë¦¬
    categorical_columns = ['embarked', 'embark_town', 'deck']
    for col in categorical_columns:
        if col in data.columns:
            # ë²”ì£¼í˜•ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•œ í›„ ê²°ì¸¡ê°’ ì²˜ë¦¬
            data[col] = data[col].astype(str)
            data[col] = data[col].replace('nan', 'Missing')
            data[col] = data[col].fillna('Missing')
    
    # ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’ì€ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
    data['age'] = data['age'].fillna(data['age'].mean())
    
    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ì •ë³´)
    drop_cols = ['alive', 'who', 'class']
    data = data.drop(columns=[col for col in drop_cols if col in data.columns])
    
    return data

# ============================================================================
# 3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ë…¼ë¬¸ì˜ ê³ ê°/ì†¡ì¥ ë ˆë²¨ íŠ¹ì§• ëª¨ë°©)
# ============================================================================

def create_features(df):
    """
    ë…¼ë¬¸ì˜ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë°©ì‹ì„ íƒ€ì´íƒ€ë‹‰ì— ì ìš©
    ê³ ê° ë ˆë²¨ íŠ¹ì§•ê³¼ ê°œë³„ ë ˆì½”ë“œ íŠ¹ì§•ì„ ìƒì„±
    """
    data = df.copy()
    
    # ê³ ê° ë ˆë²¨ íŠ¹ì§• (pclassë¥¼ ê³ ê° ê·¸ë£¹ìœ¼ë¡œ ê°€ì •)
    customer_features = data.groupby('pclass').agg({
        'fare': ['mean', 'std', 'count'],
        'survived': ['mean', 'count'],
        'age': ['mean', 'std']
    }).round(3)
    
    customer_features.columns = ['_'.join(col) for col in customer_features.columns]
    customer_features = customer_features.add_prefix('customer_')
    
    # ì›ë³¸ ë°ì´í„°ì— ê³ ê° ë ˆë²¨ íŠ¹ì§• ê²°í•©
    data = data.merge(customer_features, left_on='pclass', right_index=True, how='left')
    
    # ì¶”ê°€ íŒŒìƒ ë³€ìˆ˜ ìƒì„± (ë…¼ë¬¸ì˜ ìƒí˜¸ì‘ìš© íŠ¹ì§• ëª¨ë°©)
    data['family_size'] = data['sibsp'] + data['parch'] + 1
    data['is_alone'] = (data['family_size'] == 1).astype(int)
    data['fare_per_person'] = data['fare'] / data['family_size']
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    data['fare_ratio'] = np.where(data['customer_fare_mean'] > 0, 
                                  data['fare'] / data['customer_fare_mean'], 0)
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    le_dict = {}
    categorical_cols = ['sex', 'embarked', 'embark_town', 'deck']
    for col in categorical_cols:
        if col in data.columns:
            le_dict[col] = LabelEncoder()
            data[f'{col}_encoded'] = le_dict[col].fit_transform(data[col].astype(str))
    
    return data, le_dict

# ============================================================================
# 4. ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„± (ë…¼ë¬¸ì˜ 5ê°œ ì§€ì—° êµ¬ê°„ ëª¨ë°©)
# ============================================================================

def create_multilabel_target(df, target_col='survived'):
    """
    ë…¼ë¬¸ì²˜ëŸ¼ ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿì„ ìƒì„±
    ìƒì¡´ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¬¸ì œë¡œ ë³€í™˜
    """
    data = df.copy()
    
    # ìƒì¡´ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë ˆë²¨ ìƒì„± (ë…¼ë¬¸ì˜ ì§€ì—° ê¸°ê°„ ëª¨ë°©)
    risk_features = ['age', 'fare', 'pclass', 'family_size', 'customer_survived_mean']
    
    # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ê³„ì‚° (ì •ê·œí™”ëœ ê°€ì¤‘í•©)
    scaler = StandardScaler()
    risk_data = scaler.fit_transform(data[risk_features])
    
    # ê°€ì¤‘ì¹˜ (ë‚˜ì´ëŠ” ë†’ì„ìˆ˜ë¡, ìš”ê¸ˆì€ ë‚®ì„ìˆ˜ë¡, í´ë˜ìŠ¤ëŠ” ë†’ì„ìˆ˜ë¡ ìœ„í—˜)
    weights = np.array([0.3, -0.2, 0.3, 0.1, -0.1])  
    risk_score = np.dot(risk_data, weights)
    
    # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ë¥¼ 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ë…¼ë¬¸ì˜ 5ê°œ ì§€ì—° êµ¬ê°„ ëª¨ë°©)
    risk_percentiles = np.percentile(risk_score, [20, 40, 60, 80])
    
    def assign_risk_class(score):
        if score <= risk_percentiles[0]:
            return 0  # Very Low Risk (ë…¼ë¬¸ì˜ 'paid on time')
        elif score <= risk_percentiles[1]:
            return 1  # Low Risk (ë…¼ë¬¸ì˜ 'gt_5_lt_30')
        elif score <= risk_percentiles[2]:
            return 2  # Medium Risk (ë…¼ë¬¸ì˜ 'gt_30_lt_60')
        elif score <= risk_percentiles[3]:
            return 3  # High Risk (ë…¼ë¬¸ì˜ 'gt_60_lt_90')
        else:
            return 4  # Very High Risk (ë…¼ë¬¸ì˜ 'gt_90')
    
    data['risk_class'] = np.array([assign_risk_class(score) for score in risk_score])
    
    return data, risk_percentiles

# ============================================================================
# 5. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë¶„ë¥˜ê¸° êµ¬í˜„ (ë…¼ë¬¸ì˜ 2ì¸µ êµ¬ì¡°)
# ============================================================================

class StackedEnsembleClassifier(BaseEstimator, ClassifierMixin):
    """
    ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ 2ì¸µ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë¶„ë¥˜ê¸°
    Base Layer: 4ê°œì˜ One-vs-Rest ì´ì§„ ë¶„ë¥˜ê¸°
    Meta Layer: Random Forest ìµœì¢… ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, base_classifiers=None, meta_classifier=None, use_probabilities=True):
        self.base_classifiers = base_classifiers if base_classifiers else [
            RandomForestClassifier(n_estimators=50, random_state=42),
            GradientBoostingClassifier(n_estimators=50, random_state=42),
            ExtraTreesClassifier(n_estimators=50, random_state=42),
        ]
        self.meta_classifier = meta_classifier if meta_classifier else RandomForestClassifier(
            n_estimators=100, random_state=42
        )
        self.use_probabilities = use_probabilities
        self.classes_ = None
        
    def create_base_targets(self, y):
        """
        ë…¼ë¬¸ì˜ 4ê°œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„± (lt_X_gt_X ë°©ì‹)
        ê° í´ë˜ìŠ¤ì— ëŒ€í•´ One-vs-Rest ë°©ì‹ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        """
        base_targets = {}
        
        # ê° í´ë˜ìŠ¤ì— ëŒ€í•´ One-vs-Rest ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        for class_idx in [0, 1, 2, 3]:  # 0, 1, 2, 3ë²ˆ í´ë˜ìŠ¤ (4ë²ˆì€ ì œì™¸í•˜ì—¬ ê· í˜•ë§ì¶¤)
            name = f'class_{class_idx}_vs_rest'
            base_targets[name] = (y == class_idx).astype(int)
            
        return base_targets
    
    def fit(self, X, y):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í•™ìŠµ"""
        self.classes_ = np.unique(y)
        
        # Base classifiersë¥¼ ìœ„í•œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        base_targets = self.create_base_targets(y)
        
        # Base classifiers í•™ìŠµ ë° ì˜ˆì¸¡
        base_predictions = []
        self.fitted_base_classifiers = []
        
        for i, (name, target) in enumerate(base_targets.items()):
            print(f"Training base classifier {i+1}/4: {name}")
            print(f"  Class distribution: {np.bincount(target)}")
            
            # ëª¨ë“  ìƒ˜í”Œì´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì—ë§Œ ì†í•˜ëŠ” ê²½ìš° ìŠ¤í‚µ
            if len(np.unique(target)) < 2:
                print(f"  Skipping {name} - only one class present")
                continue
                
            # ê° base classifier í•™ìŠµ
            clf_idx = i % len(self.base_classifiers)  # ë¶„ë¥˜ê¸° ìˆœí™˜ ì‚¬ìš©
            clf_params = self.base_classifiers[clf_idx].get_params()
            clf = type(self.base_classifiers[clf_idx])(**clf_params)
            clf.fit(X, target)
            self.fitted_base_classifiers.append(clf)
            
            # Cross-validationìœ¼ë¡œ meta-features ìƒì„±
            pred_labels = cross_val_predict(clf, X, target, cv=3, method='predict')
            
            if self.use_probabilities and hasattr(clf, 'predict_proba'):
                pred_proba = cross_val_predict(clf, X, target, cv=3, method='predict_proba')
                # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
                if pred_proba.shape[1] == 2:
                    base_pred = np.column_stack([pred_labels.reshape(-1, 1), pred_proba[:, 1].reshape(-1, 1)])
                else:
                    base_pred = pred_labels.reshape(-1, 1)
            else:
                base_pred = pred_labels.reshape(-1, 1)
            
            base_predictions.append(base_pred)
        
        # Meta features ìƒì„±
        if base_predictions:
            meta_features = np.hstack(base_predictions)
            print(f"Meta features shape: {meta_features.shape}")
            
            # Meta classifier í•™ìŠµ
            self.meta_classifier.fit(meta_features, y)
        else:
            raise ValueError("No valid base classifiers were trained")
        
        return self
    
    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        # Base classifiersë¡œ ì˜ˆì¸¡
        base_predictions = []
        
        for clf in self.fitted_base_classifiers:
            pred_labels = clf.predict(X)
            
            if self.use_probabilities and hasattr(clf, 'predict_proba'):
                pred_proba = clf.predict_proba(X)
                if pred_proba.shape[1] == 2:
                    base_pred = np.column_stack([pred_labels.reshape(-1, 1), pred_proba[:, 1].reshape(-1, 1)])
                else:
                    base_pred = pred_labels.reshape(-1, 1)
            else:
                base_pred = pred_labels.reshape(-1, 1)
                
            base_predictions.append(base_pred)
        
        # Meta features ìƒì„±
        meta_features = np.hstack(base_predictions)
        
        # Meta classifierë¡œ ìµœì¢… ì˜ˆì¸¡
        return self.meta_classifier.predict(meta_features)

# ============================================================================
# 6. ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ
# ============================================================================

def evaluate_models(X_train, X_test, y_train, y_test):
    """
    ì§ì ‘ ë¶„ë¥˜ì™€ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ
    """
    
    print("=== ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ===")
    
    # 1. ì§ì ‘ ë¶„ë¥˜ (Single-layer) - ë…¼ë¬¸ì˜ baseline
    print("\n1. ì§ì ‘ ë¶„ë¥˜ (Baseline)")
    direct_classifier = RandomForestClassifier(n_estimators=200, random_state=42)
    direct_classifier.fit(X_train, y_train)
    y_pred_direct = direct_classifier.predict(X_test)
    direct_accuracy = accuracy_score(y_test, y_pred_direct)
    
    print(f"ì§ì ‘ ë¶„ë¥˜ ì •í™•ë„: {direct_accuracy:.3f}")
    
    # 2. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (Two-layer) - ë…¼ë¬¸ì˜ ì œì•ˆ ë°©ë²•
    print("\n2. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (ì œì•ˆ ë°©ë²•)")
    stacked_classifier = StackedEnsembleClassifier(use_probabilities=True)
    stacked_classifier.fit(X_train, y_train)
    y_pred_stacked = stacked_classifier.predict(X_test)
    stacked_accuracy = accuracy_score(y_test, y_pred_stacked)
    
    print(f"\nìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì •í™•ë„: {stacked_accuracy:.3f}")
    
    # ì„±ëŠ¥ ë¹„êµ
    print(f"\n=== ì„±ëŠ¥ ë¹„êµ ===")
    print(f"ì§ì ‘ ë¶„ë¥˜ ì •í™•ë„:     {direct_accuracy:.3f}")
    print(f"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì •í™•ë„: {stacked_accuracy:.3f}")
    print(f"ê°œì„  ì •ë„:           {(stacked_accuracy - direct_accuracy)*100:+.1f}%p")
    
    # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    target_names = ['Very Low Risk', 'Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']
    
    print(f"\n=== ì§ì ‘ ë¶„ë¥˜ ìƒì„¸ ê²°ê³¼ ===")
    print(classification_report(y_test, y_pred_direct, target_names=target_names))
    
    print(f"\n=== ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ìƒì„¸ ê²°ê³¼ ===")
    print(classification_report(y_test, y_pred_stacked, target_names=target_names))
    
    return direct_classifier, stacked_classifier, y_pred_direct, y_pred_stacked, direct_accuracy, stacked_accuracy

# ============================================================================
# 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ëŒ€ì²´)
# ============================================================================

def analyze_feature_importance(model, X, feature_names):
    """
    íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ê¸°ëŠ¥ ëŒ€ì²´)
    """
    if hasattr(model, 'feature_importances_'):
        feature_importance = model.feature_importances_
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print("=== íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ëŒ€ì²´) ===")
        print("ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):
            print(f"{i+1:2d}. {row['feature']:20s}: {row['importance']:.3f}")
        
        return importance_df
    else:
        print("í•´ë‹¹ ëª¨ë¸ì€ feature_importances_ ì†ì„±ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

# ============================================================================
# 8. ê²°ê³¼ ì €ì¥
# ============================================================================

def save_results(y_test, y_pred_direct, y_pred_stacked, importance_df, direct_accuracy, stacked_accuracy):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    
    # 1. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame({
        'actual_risk_class': y_test,
        'direct_prediction': y_pred_direct,
        'stacked_prediction': y_pred_stacked,
        'direct_correct': (y_test == y_pred_direct).astype(int),
        'stacked_correct': (y_test == y_pred_stacked).astype(int)
    })
    results_df.to_csv('titanic_stacking_predictions.csv', index=False)
    
    # 2. íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
    if importance_df is not None:
        importance_df.to_csv('titanic_feature_importance.csv', index=False)
    
    # 3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì €ì¥
    performance_df = pd.DataFrame({
        'Model': ['Direct Classification', 'Stacked Ensemble'],
        'Accuracy': [direct_accuracy, stacked_accuracy],
        'Improvement_vs_Direct': [0, stacked_accuracy - direct_accuracy]
    })
    performance_df.to_csv('titanic_model_performance.csv', index=False)
    
    print("\n=== CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ ===")
    print("1. titanic_stacking_predictions.csv - ì˜ˆì¸¡ ê²°ê³¼")
    print("2. titanic_feature_importance.csv - íŠ¹ì„± ì¤‘ìš”ë„")  
    print("3. titanic_model_performance.csv - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

# ============================================================================
# 9. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë…¼ë¬¸ êµ¬í˜„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    print("="*80)
    print("ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡")
    print("ë…¼ë¬¸ ë°©ë²•ë¡  êµ¬í˜„ ë° íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì ìš©")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    titanic = load_and_explore_data()
    
    # 2. ì „ì²˜ë¦¬
    print("\n" + "="*50)
    print("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    titanic_processed = preprocess_titanic_data(titanic)
    
    # 3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
    titanic_features, label_encoders = create_features(titanic_processed)
    
    # 4. ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„±
    print("ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„± ì¤‘...")
    titanic_multilabel, risk_thresholds = create_multilabel_target(titanic_features)
    
    # 5. íŠ¹ì§• ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„
    feature_columns = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'family_size', 'is_alone', 
                       'fare_per_person', 'fare_ratio', 'customer_fare_mean', 'customer_fare_std',
                       'customer_survived_mean', 'sex_encoded', 'embarked_encoded', 
                       'embark_town_encoded', 'deck_encoded', 'adult_male']
    
    X = titanic_multilabel[feature_columns].fillna(0)
    y = titanic_multilabel['risk_class']
    
    print(f"\nìµœì¢… ë°ì´í„° í˜•íƒœ:")
    print(f"- íŠ¹ì§• ìˆ˜: {X.shape[1]}")
    print(f"- ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    print(f"- íƒ€ê²Ÿ í´ë˜ìŠ¤: {np.unique(y)}")
    
    # 6. ë°ì´í„° ë¶„í•  (ë…¼ë¬¸ì²˜ëŸ¼ 80:20 ë¹„ìœ¨)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\në°ì´í„° ë¶„í• :")
    print(f"- í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ìƒ˜í”Œ")
    print(f"- í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ")
    
    # 7. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    print("\n" + "="*50)
    results = evaluate_models(X_train, X_test, y_train, y_test)
    direct_classifier, stacked_classifier, y_pred_direct, y_pred_stacked, direct_accuracy, stacked_accuracy = results
    
    # 8. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    print("\n" + "="*50)
    importance_df = analyze_feature_importance(direct_classifier, X, feature_columns)
    
    # 9. ê²°ê³¼ ì €ì¥
    save_results(y_test, y_pred_direct, y_pred_stacked, importance_df, direct_accuracy, stacked_accuracy)
    
    # 10. ìµœì¢… ìš”ì•½
    print(f"\n" + "="*80)
    print(f"ğŸ“‹ ë…¼ë¬¸ êµ¬í˜„ ë° íƒ€ì´íƒ€ë‹‰ ì ìš© ìµœì¢… ìš”ì•½")
    print(f"="*80)
    
    print(f"\nğŸ¯ ë…¼ë¬¸ ì£¼ìš” ë‚´ìš©:")
    print(f"- ì œëª©: ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡")
    print(f"- ë°©ë²•ë¡ : 2ì¸µ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (Base Classifiers + Meta Classifier)")
    print(f"- í•µì‹¬ ê¸°ë²•: One-vs-Rest ì´ì§„ ë¶„ë¥˜ + Random Forest ë©”íƒ€ ë¶„ë¥˜ê¸°")
    print(f"- ì„¤ëª…ê°€ëŠ¥ì„±: SHAPì„ í†µí•œ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
    
    print(f"\nğŸ“Š ë…¼ë¬¸ vs íƒ€ì´íƒ€ë‹‰ ì ìš© ê²°ê³¼:")
    print(f"ë…¼ë¬¸ (ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ë°ì´í„°):")
    print(f"  â€¢ ë°ì´í„°: 107,012ê°œ ì†¡ì¥, 37ê°œ íŠ¹ì„±")
    print(f"  â€¢ ì§ì ‘ ë¶„ë¥˜: 81% ì •í™•ë„")
    print(f"  â€¢ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”: 93% ì •í™•ë„ (+12%p)")
    
    print(f"\níƒ€ì´íƒ€ë‹‰ ì ìš©:")  
    print(f"  â€¢ ë°ì´í„°: 891ê°œ ìŠ¹ê°, 17ê°œ íŠ¹ì„±")
    print(f"  â€¢ ì§ì ‘ ë¶„ë¥˜: {direct_accuracy:.1%} ì •í™•ë„")
    print(f"  â€¢ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”: {stacked_accuracy:.1%} ì •í™•ë„ ({(stacked_accuracy-direct_accuracy)*100:+.1f}%p)")
    
    print(f"\nğŸ” ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:")
    print(f"1. ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ì€ ëŒ€ìš©ëŸ‰ ë³µì¡ ë°ì´í„°ì—ì„œ ì§„ê°€ë¥¼ ë°œíœ˜")
    print(f"2. ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ë‹¨ìˆœí•œ ëª¨ë¸ì´ ë•Œë¡œëŠ” ë” íš¨ê³¼ì ")
    print(f"3. ë„ë©”ì¸ íŠ¹ì„±ì„ ë°˜ì˜í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì´ í•µì‹¬")
    print(f"4. ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ìš©í•  ë•ŒëŠ” ì ì ˆí•œ ì¡°ì • í•„ìš”")
    
    print(f"\nâœ… êµ¬í˜„ ì™„ë£Œ!")

# ============================================================================
# 10. ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()