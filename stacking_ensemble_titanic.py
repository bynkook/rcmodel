# ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡ - ë…¼ë¬¸ êµ¬í˜„ ì½”ë“œ
# íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ì ìš©

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold, cross_val_predict
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, RocCurveDisplay, roc_curve, auc, roc_auc_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize
from sklearn.base import BaseEstimator, ClassifierMixin
from itertools import cycle
import shap
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ í™•ì¸
# ============================================================================

def load_and_explore_data():
    """íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë¡œë“œ ë° ê¸°ë³¸ íƒìƒ‰"""
    # íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë¡œë“œ
    titanic = sns.load_dataset('titanic')
    
    print("=== íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ ===")
    print(f"ë°ì´í„° í˜•íƒœ: {titanic.shape}")
    print(f"\nì»¬ëŸ¼ ì •ë³´:")
    print(titanic.info())
    
    print(f"\nê²°ì¸¡ê°’ í™•ì¸:")
    print(titanic.isnull().sum())
    
    print(f"\nìƒì¡´ë¥  ë¶„í¬:")
    print(titanic['survived'].value_counts(normalize=True))
    
    return titanic

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ (ë…¼ë¬¸ ë°©ì‹ ì ìš©)
# ============================================================================

def preprocess_titanic_data(df):
    """
    ë…¼ë¬¸ì˜ ì „ì²˜ë¦¬ ë°©ì‹ì„ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì— ì ìš©
    - ê²°ì¸¡ê°’ì„ 'Missing' ë¬¸ìì—´ë¡œ ëŒ€ì²´
    - ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
    """
    data = df.copy()
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ì¼ë°˜ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ê²°ì¸¡ê°’ ì²˜ë¦¬
    categorical_columns = ['embarked', 'embark_town', 'deck']
    for col in categorical_columns:
        if col in data.columns:
            # ë²”ì£¼í˜•ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•œ í›„ ê²°ì¸¡ê°’ ì²˜ë¦¬
            data[col] = data[col].astype(str)
            data[col] = data[col].replace('nan', 'Missing')
            data[col] = data[col].fillna('Missing')
    
    # ìˆ˜ì¹˜í˜• ê²°ì¸¡ê°’ì€ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
    data['age'] = data['age'].fillna(data['age'].mean())
    
    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (ì¤‘ë³µ ì •ë³´)
    drop_cols = ['alive', 'who', 'class']
    data = data.drop(columns=[col for col in drop_cols if col in data.columns])
    
    return data

# ============================================================================
# 3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ë…¼ë¬¸ì˜ ê³ ê°/ì†¡ì¥ ë ˆë²¨ íŠ¹ì§• ëª¨ë°©)
# ============================================================================

def create_features(df):
    """
    ë…¼ë¬¸ì˜ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ë°©ì‹ì„ íƒ€ì´íƒ€ë‹‰ì— ì ìš©
    ê³ ê° ë ˆë²¨ íŠ¹ì§•ê³¼ ê°œë³„ ë ˆì½”ë“œ íŠ¹ì§•ì„ ìƒì„±
    """
    data = df.copy()
    
    # ê³ ê° ë ˆë²¨ íŠ¹ì§• (pclassë¥¼ ê³ ê° ê·¸ë£¹ìœ¼ë¡œ ê°€ì •)
    customer_features = data.groupby('pclass').agg({
        'fare': ['mean', 'std', 'count'],
        'survived': ['mean', 'count'],
        'age': ['mean', 'std']
    }).round(3)
    
    customer_features.columns = ['_'.join(col) for col in customer_features.columns]
    customer_features = customer_features.add_prefix('customer_')
    
    # ì›ë³¸ ë°ì´í„°ì— ê³ ê° ë ˆë²¨ íŠ¹ì§• ê²°í•©
    data = data.merge(customer_features, left_on='pclass', right_index=True, how='left')
    
    # ì¶”ê°€ íŒŒìƒ ë³€ìˆ˜ ìƒì„± (ë…¼ë¬¸ì˜ ìƒí˜¸ì‘ìš© íŠ¹ì§• ëª¨ë°©)
    data['family_size'] = data['sibsp'] + data['parch'] + 1
    data['is_alone'] = (data['family_size'] == 1).astype(int)
    data['fare_per_person'] = data['fare'] / data['family_size']
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    data['fare_ratio'] = np.where(data['customer_fare_mean'] > 0, 
                                  data['fare'] / data['customer_fare_mean'], 0)
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    le_dict = {}
    categorical_cols = ['sex', 'embarked', 'embark_town', 'deck']
    for col in categorical_cols:
        if col in data.columns:
            le_dict[col] = LabelEncoder()
            data[f'{col}_encoded'] = le_dict[col].fit_transform(data[col].astype(str))
    
    return data, le_dict

# ============================================================================
# 4. ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„± (ë…¼ë¬¸ì˜ 5ê°œ ì§€ì—° êµ¬ê°„ ëª¨ë°©)
# ============================================================================

def create_multilabel_target(df, target_col='survived'):
    """
    ë…¼ë¬¸ì²˜ëŸ¼ ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿì„ ìƒì„±
    ìƒì¡´ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¬¸ì œë¡œ ë³€í™˜
    """
    data = df.copy()
    
    # ìƒì¡´ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë ˆë²¨ ìƒì„± (ë…¼ë¬¸ì˜ ì§€ì—° ê¸°ê°„ ëª¨ë°©)
    risk_features = ['age', 'fare', 'pclass', 'family_size', 'customer_survived_mean']
    
    # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ê³„ì‚° (ì •ê·œí™”ëœ ê°€ì¤‘í•©)
    scaler = StandardScaler()
    risk_data = scaler.fit_transform(data[risk_features])
    
    # ê°€ì¤‘ì¹˜ (ë‚˜ì´ëŠ” ë†’ì„ìˆ˜ë¡, ìš”ê¸ˆì€ ë‚®ì„ìˆ˜ë¡, í´ë˜ìŠ¤ëŠ” ë†’ì„ìˆ˜ë¡ ìœ„í—˜)
    weights = np.array([0.3, -0.2, 0.3, 0.1, -0.1])  
    risk_score = np.dot(risk_data, weights)
    
    # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ë¥¼ 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ë…¼ë¬¸ì˜ 5ê°œ ì§€ì—° êµ¬ê°„ ëª¨ë°©)
    risk_percentiles = np.percentile(risk_score, [20, 40, 60, 80])
    
    def assign_risk_class(score):
        if score <= risk_percentiles[0]:
            return 0  # Very Low Risk (ë…¼ë¬¸ì˜ 'paid on time')
        elif score <= risk_percentiles[1]:
            return 1  # Low Risk (ë…¼ë¬¸ì˜ 'gt_5_lt_30')
        elif score <= risk_percentiles[2]:
            return 2  # Medium Risk (ë…¼ë¬¸ì˜ 'gt_30_lt_60')
        elif score <= risk_percentiles[3]:
            return 3  # High Risk (ë…¼ë¬¸ì˜ 'gt_60_lt_90')
        else:
            return 4  # Very High Risk (ë…¼ë¬¸ì˜ 'gt_90')
    
    data['risk_class'] = np.array([assign_risk_class(score) for score in risk_score])
    
    return data, risk_percentiles

# ============================================================================
# 5. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë¶„ë¥˜ê¸° êµ¬í˜„ (ë…¼ë¬¸ì˜ 2ì¸µ êµ¬ì¡°)
# ============================================================================

class StackedEnsembleClassifier(BaseEstimator, ClassifierMixin):
    """
    ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ 2ì¸µ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë¶„ë¥˜ê¸°
    Base Layer: 4ê°œì˜ One-vs-Rest ì´ì§„ ë¶„ë¥˜ê¸°
    Meta Layer: Random Forest ìµœì¢… ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, base_classifiers=None, meta_classifier=None, use_probabilities=True):
        self.base_classifiers = base_classifiers if base_classifiers else [
            RandomForestClassifier(n_estimators=50, random_state=42),
            GradientBoostingClassifier(n_estimators=50, random_state=42),
            ExtraTreesClassifier(n_estimators=50, random_state=42),
        ]
        self.meta_classifier = meta_classifier if meta_classifier else RandomForestClassifier(
            n_estimators=100, random_state=42
        )
        self.use_probabilities = use_probabilities
        self.classes_ = None
        
    def create_base_targets(self, y):
        """
        ë…¼ë¬¸ì˜ 4ê°œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„± (lt_X_gt_X ë°©ì‹)
        ê° í´ë˜ìŠ¤ì— ëŒ€í•´ One-vs-Rest ë°©ì‹ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        """
        base_targets = {}
        
        # ê° í´ë˜ìŠ¤ì— ëŒ€í•´ One-vs-Rest ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        for class_idx in [0, 1, 2, 3]:  # 0, 1, 2, 3ë²ˆ í´ë˜ìŠ¤ (4ë²ˆì€ ì œì™¸í•˜ì—¬ ê· í˜•ë§ì¶¤)
            name = f'class_{class_idx}_vs_rest'
            base_targets[name] = (y == class_idx).astype(int)
            
        return base_targets
    
    def fit(self, X, y):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í•™ìŠµ"""
        self.classes_ = np.unique(y)
        
        # Base classifiersë¥¼ ìœ„í•œ ì´ì§„ ë¶„ë¥˜ íƒ€ê²Ÿ ìƒì„±
        base_targets = self.create_base_targets(y)
        
        # Base classifiers í•™ìŠµ ë° ì˜ˆì¸¡
        base_predictions = []
        self.fitted_base_classifiers = []
        
        for i, (name, target) in enumerate(base_targets.items()):
            print(f"Training base classifier {i+1}/4: {name}")
            print(f"  Class distribution: {np.bincount(target)}")
            
            # ëª¨ë“  ìƒ˜í”Œì´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì—ë§Œ ì†í•˜ëŠ” ê²½ìš° ìŠ¤í‚µ
            if len(np.unique(target)) < 2:
                print(f"  Skipping {name} - only one class present")
                continue
                
            # ê° base classifier í•™ìŠµ
            clf_idx = i % len(self.base_classifiers)  # ë¶„ë¥˜ê¸° ìˆœí™˜ ì‚¬ìš©
            clf_params = self.base_classifiers[clf_idx].get_params()
            clf = type(self.base_classifiers[clf_idx])(**clf_params)
            clf.fit(X, target)
            self.fitted_base_classifiers.append(clf)
            
            # Cross-validationìœ¼ë¡œ meta-features ìƒì„±
            pred_labels = cross_val_predict(clf, X, target, cv=3, method='predict')
            
            if self.use_probabilities and hasattr(clf, 'predict_proba'):
                pred_proba = cross_val_predict(clf, X, target, cv=3, method='predict_proba')
                # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ ë§Œ ì‚¬ìš©
                if pred_proba.shape[1] == 2:
                    base_pred = np.column_stack([pred_labels.reshape(-1, 1), pred_proba[:, 1].reshape(-1, 1)])
                else:
                    base_pred = pred_labels.reshape(-1, 1)
            else:
                base_pred = pred_labels.reshape(-1, 1)
            
            base_predictions.append(base_pred)
        
        # Meta features ìƒì„±
        if base_predictions:
            meta_features = np.hstack(base_predictions)
            print(f"Meta features shape: {meta_features.shape}")
            
            # Meta classifier í•™ìŠµ
            self.meta_classifier.fit(meta_features, y)
        else:
            raise ValueError("No valid base classifiers were trained")
        
        return self
    
    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        # Base classifiersë¡œ ì˜ˆì¸¡
        base_predictions = []
        
        for clf in self.fitted_base_classifiers:
            pred_labels = clf.predict(X)
            
            if self.use_probabilities and hasattr(clf, 'predict_proba'):
                pred_proba = clf.predict_proba(X)
                if pred_proba.shape[1] == 2:
                    base_pred = np.column_stack([pred_labels.reshape(-1, 1), pred_proba[:, 1].reshape(-1, 1)])
                else:
                    base_pred = pred_labels.reshape(-1, 1)
            else:
                base_pred = pred_labels.reshape(-1, 1)
                
            base_predictions.append(base_pred)
        
        # Meta features ìƒì„±
        meta_features = np.hstack(base_predictions)
        
        # Meta classifierë¡œ ìµœì¢… ì˜ˆì¸¡
        return self.meta_classifier.predict(meta_features)

# ============================================================================
# 6. ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ
# ============================================================================

def evaluate_models(X_train, X_test, y_train, y_test):
    """
    ì§ì ‘ ë¶„ë¥˜ì™€ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ
    """
    
    print("=== ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ===")
    
    # 1. ì§ì ‘ ë¶„ë¥˜ (Single-layer) - ë…¼ë¬¸ì˜ baseline
    print("\n1. ì§ì ‘ ë¶„ë¥˜ (Baseline)")
    direct_classifier = RandomForestClassifier(n_estimators=200, random_state=42)
    direct_classifier.fit(X_train, y_train)
    y_pred_direct = direct_classifier.predict(X_test)
    direct_accuracy = accuracy_score(y_test, y_pred_direct)
    
    print(f"ì§ì ‘ ë¶„ë¥˜ ì •í™•ë„: {direct_accuracy:.3f}")
    
    # 2. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (Two-layer) - ë…¼ë¬¸ì˜ ì œì•ˆ ë°©ë²•
    print("\n2. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (ì œì•ˆ ë°©ë²•)")
    stacked_classifier = StackedEnsembleClassifier(use_probabilities=True)
    stacked_classifier.fit(X_train, y_train)
    y_pred_stacked = stacked_classifier.predict(X_test)
    stacked_accuracy = accuracy_score(y_test, y_pred_stacked)
    
    print(f"\nìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì •í™•ë„: {stacked_accuracy:.3f}")
    
    # ì„±ëŠ¥ ë¹„êµ
    print(f"\n=== ì„±ëŠ¥ ë¹„êµ ===")
    print(f"ì§ì ‘ ë¶„ë¥˜ ì •í™•ë„:     {direct_accuracy:.3f}")
    print(f"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì •í™•ë„: {stacked_accuracy:.3f}")
    print(f"ê°œì„  ì •ë„:           {(stacked_accuracy - direct_accuracy)*100:+.1f}%p")
    
    # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    target_names = ['Very Low Risk', 'Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']
    
    print(f"\n=== ì§ì ‘ ë¶„ë¥˜ ìƒì„¸ ê²°ê³¼ ===")
    print(classification_report(y_test, y_pred_direct, target_names=target_names))
    
    print(f"\n=== ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ìƒì„¸ ê²°ê³¼ ===")
    print(classification_report(y_test, y_pred_stacked, target_names=target_names))
    
    return direct_classifier, stacked_classifier, y_pred_direct, y_pred_stacked, direct_accuracy, stacked_accuracy

# ============================================================================
# 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ëŒ€ì²´)
# ============================================================================

def analyze_feature_importance(model, X, feature_names):
    """
    íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ê¸°ëŠ¥ ëŒ€ì²´)
    """
    if hasattr(model, 'feature_importances_'):
        feature_importance = model.feature_importances_
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print("=== íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë…¼ë¬¸ì˜ SHAP ëŒ€ì²´) ===")
        print("ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):
            print(f"{i+1:2d}. {row['feature']:20s}: {row['importance']:.3f}")
        
        return importance_df
    else:
        print("í•´ë‹¹ ëª¨ë¸ì€ feature_importances_ ì†ì„±ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

# ROC, AUC
def plot_roc_auc(model, X, y, class_names=None, title='ROC Curve', savepath=None):
    """
    Binary/Multiclass ROC curve and AUC.
    Parameters
    ----------
    model : fitted estimator with predict_proba or decision_function
    X : array-like, shape (n_samples, n_features)
    y : array-like, shape (n_samples,)
    class_names : list[str] | None
    title : str
    savepath : str | None
    Returns
    -------
    dict : {'micro': auc or None, <class_name>: auc, ...}
    """
    y = np.asarray(y)
    classes = np.unique(y)
    n_classes = len(classes)
    if class_names is None:
        class_names = [str(c) for c in classes]

    # score matrix
    if hasattr(model, "predict_proba"):
        scores = model.predict_proba(X)
        if n_classes == 2:
            # ensure shape (n_samples,)
            scores = scores[:, 1] if scores.ndim == 2 else scores.ravel()
    elif hasattr(model, "decision_function"):
        scores = model.decision_function(X)
        if n_classes == 2 and scores.ndim > 1:
            scores = scores.ravel()
    else:
        raise ValueError("Estimator must implement predict_proba or decision_function.")

    fig, ax = plt.subplots(figsize=(7, 5))
    auc_map = {}
    if n_classes == 2:
        fpr, tpr, _ = roc_curve(y, scores, pos_label=classes[1])
        roc_auc = auc(fpr, tpr)
        auc_map[class_names[1]] = roc_auc
        ax.plot(fpr, tpr, label=f"AUC = {roc_auc:.4f}")
        ax.plot([0, 1], [0, 1], linestyle="--")
        ax.set_xlabel("False Positive Rate")
        ax.set_ylabel("True Positive Rate")
        ax.set_title(title)
        ax.legend(loc="lower right")
    else:
        y_bin = label_binarize(y, classes=classes)
        if scores.ndim == 1:
            raise ValueError("For multiclass ROC, score must be 2D with class probabilities or decision scores.")
        fpr, tpr = {}, {}
        for i, cname in enumerate(class_names):
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], scores[:, i])
            auc_i = auc(fpr[i], tpr[i])
            auc_map[cname] = auc_i
            ax.plot(fpr[i], tpr[i], label=f"{cname} AUC = {auc_i:.4f}")
        fpr["micro"], tpr["micro"], _ = roc_curve(y_bin.ravel(), scores.ravel())
        auc_micro = auc(fpr["micro"], tpr["micro"])
        auc_map["micro"] = auc_micro
        ax.plot(fpr["micro"], tpr["micro"], label=f"micro-average AUC = {auc_micro:.4f}")
        ax.plot([0, 1], [0, 1], linestyle="--")
        ax.set_xlabel("False Positive Rate")
        ax.set_ylabel("True Positive Rate")
        ax.set_title(title)
        ax.legend(loc="lower right")
    # ensure nothing is clipped on the canvas
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    fig.tight_layout()  # adjust paddings
    if savepath:
        fig.savefig(savepath, bbox_inches="tight", dpi=150)
    plt.show()
    return auc_map

# SHAP ë¶„ì„
def analyze_feature_importance_shap(model, X, feature_names=None, class_idx=None, max_display=20, title_prefix="SHAP", savepath_prefix=None):
    """
    SHAP analysis and plotting without touching existing analyze_feature_importance().
    Parameters
    ----------
    model : fitted estimator
    X : pandas.DataFrame or np.ndarray
    feature_names : list[str] | None
    class_idx : int | None  # for classification, which class to visualize
    max_display : int
    title_prefix : str
    savepath_prefix : str | None  # if given, save figures as f"{savepath_prefix}_beeswarm.png" and f"{savepath_prefix}_bar.png"
    Returns
    -------
    shap_values : shap.Explanation or np.ndarray (implementation-dependent)
    """
    if shap is None:
        raise ImportError("shap is not installed. Please `pip install shap`.")

    # Prepare data and names
    if hasattr(X, "values") and hasattr(X, "columns"):
        data = X
        if feature_names is None:
            feature_names = list(X.columns)
    else:
        data = np.asarray(X)
        if feature_names is None:
            feature_names = [f"f{i}" for i in range(data.shape[1])]

    # Prefer TreeExplainer when possible, else fallback
    try:
        explainer = shap.TreeExplainer(model)
    except Exception:
        explainer = shap.Explainer(model)

    sv = explainer(data)

    # Select class for multi-output classification if needed
    # Newer SHAP returns Explanation with potentially 3D values; slice to 2D.
    try:
        values = sv.values
    except Exception:
        values = np.asarray(sv)

    selected = sv
    try:
        if values.ndim == 3:
            k = class_idx if class_idx is not None else (1 if values.shape[2] > 1 else 0)
            base_vals = getattr(sv, "base_values", None)
            base_vals_k = base_vals[:, k] if isinstance(base_vals, np.ndarray) and base_vals.ndim == 2 else base_vals
            selected = shap.Explanation(
                values=values[:, :, k],
                base_values=base_vals_k,
                data=getattr(sv, "data", data),
                feature_names=feature_names,
            )
    except Exception:
        # Fall back silently; plots below will attempt with `sv` as-is.
        selected = sv

    # Beeswarm plot
    try:
        shap.plots.beeswarm(selected, max_display=max_display, show=False)
    except Exception:
        shap.summary_plot(getattr(selected, "values", selected), data, feature_names=feature_names, show=False, max_display=max_display)
    if title_prefix:
        plt.gcf().suptitle(f"{title_prefix} - beeswarm", y=0.98)
    plt.tight_layout(rect=(0, 0, 1, 0.96))
    if savepath_prefix:
        plt.savefig(f"{savepath_prefix}_beeswarm.png", bbox_inches="tight", dpi=150)
    plt.show()

    # Bar plot (mean |SHAP|)
    try:
        shap.plots.bar(selected, max_display=max_display, show=False)
        if title_prefix:
            plt.gcf().suptitle(f"{title_prefix} - bar", y=0.98)
        plt.tight_layout(rect=(0, 0, 1, 0.96))
        if savepath_prefix:
            plt.savefig(f"{savepath_prefix}_bar.png", bbox_inches="tight", dpi=150)
        plt.show()
    except Exception:
        # Manual bar as fallback
        vals = getattr(selected, "values", selected)
        if hasattr(vals, "shape") and vals.ndim == 2:
            mean_abs = np.abs(vals).mean(axis=0)
            order = np.argsort(mean_abs)[::-1][:max_display]
            fig = plt.figure(figsize=(8, 5))
            plt.bar(range(len(order)), mean_abs[order])
            plt.xticks(range(len(order)), [feature_names[i] for i in order], rotation=60, ha="right")
            plt.ylabel("mean(|SHAP value|)")
            plt.title(f"{title_prefix} - bar")
            fig.tight_layout()
            if savepath_prefix:
                plt.savefig(f"{savepath_prefix}_bar.png", bbox_inches="tight", dpi=150)
            plt.show()
    return sv


# ============================================================================
# 8. ê²°ê³¼ ì €ì¥
# ============================================================================

def save_results(y_test, y_pred_direct, y_pred_stacked, importance_df, direct_accuracy, stacked_accuracy):
    """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    
    # 1. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame({
        'actual_risk_class': y_test,
        'direct_prediction': y_pred_direct,
        'stacked_prediction': y_pred_stacked,
        'direct_correct': (y_test == y_pred_direct).astype(int),
        'stacked_correct': (y_test == y_pred_stacked).astype(int)
    })
    results_df.to_csv('titanic_stacking_predictions.csv', index=False)
    
    # 2. íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
    if importance_df is not None:
        importance_df.to_csv('titanic_feature_importance.csv', index=False)
    
    # 3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì €ì¥
    performance_df = pd.DataFrame({
        'Model': ['Direct Classification', 'Stacked Ensemble'],
        'Accuracy': [direct_accuracy, stacked_accuracy],
        'Improvement_vs_Direct': [0, stacked_accuracy - direct_accuracy]
    })
    performance_df.to_csv('titanic_model_performance.csv', index=False)
    
    print("\n=== CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ ===")
    print("1. titanic_stacking_predictions.csv - ì˜ˆì¸¡ ê²°ê³¼")
    print("2. titanic_feature_importance.csv - íŠ¹ì„± ì¤‘ìš”ë„")  
    print("3. titanic_model_performance.csv - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

# ============================================================================
# 9. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë…¼ë¬¸ êµ¬í˜„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    print("="*80)
    print("ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡")
    print("ë…¼ë¬¸ ë°©ë²•ë¡  êµ¬í˜„ ë° íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì ìš©")
    print("="*80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    titanic = load_and_explore_data()
    
    # 2. ì „ì²˜ë¦¬
    print("\n" + "="*50)
    print("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    titanic_processed = preprocess_titanic_data(titanic)
    
    # 3. íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    print("íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
    titanic_features, label_encoders = create_features(titanic_processed)
    
    # 4. ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„±
    print("ë‹¤ì¤‘ í´ë˜ìŠ¤ íƒ€ê²Ÿ ìƒì„± ì¤‘...")
    titanic_multilabel, risk_thresholds = create_multilabel_target(titanic_features)
    
    # 5. íŠ¹ì§• ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„
    feature_columns = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'family_size', 'is_alone', 
                       'fare_per_person', 'fare_ratio', 'customer_fare_mean', 'customer_fare_std',
                       'customer_survived_mean', 'sex_encoded', 'embarked_encoded', 
                       'embark_town_encoded', 'deck_encoded', 'adult_male']
    
    X = titanic_multilabel[feature_columns].fillna(0)
    y = titanic_multilabel['risk_class']
    
    print(f"\nìµœì¢… ë°ì´í„° í˜•íƒœ:")
    print(f"- íŠ¹ì§• ìˆ˜: {X.shape[1]}")
    print(f"- ìƒ˜í”Œ ìˆ˜: {X.shape[0]}")
    print(f"- íƒ€ê²Ÿ í´ë˜ìŠ¤: {np.unique(y)}")
    
    # 6. ë°ì´í„° ë¶„í•  (ë…¼ë¬¸ì²˜ëŸ¼ 80:20 ë¹„ìœ¨)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\në°ì´í„° ë¶„í• :")
    print(f"- í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ìƒ˜í”Œ")
    print(f"- í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ")
    
    # 7. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    print("\n" + "="*50)
    results = evaluate_models(X_train, X_test, y_train, y_test)
    direct_classifier, stacked_classifier, y_pred_direct, y_pred_stacked, direct_accuracy, stacked_accuracy = results
    
    # 8. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    print("\n" + "="*50)
    importance_df = analyze_feature_importance(direct_classifier, X, feature_columns)

    # === ROC/AUC & SHAP analysis ===    
    FINAL_MODEL_VAR = stacked_classifier
    # 1) locate fitted model variable
    frame = {**globals(), **locals()}
    model = None
    # 1-1) explicit preferred name
    if isinstance(FINAL_MODEL_VAR, str) and FINAL_MODEL_VAR:
        m = frame.get(FINAL_MODEL_VAR)
        if m is not None:
            model = m
    # 1-2) fallback: common names
    if model is None:
        for _name in ["model", "clf", "final_model", "best_model",
                      "stack_clf", "stacking_clf", "stacking_model",
                      "estimator", "pipe", "pipeline"]:
            m = frame.get(_name)
            if m is not None:
                model = m
                break
    # 1-3) last resort: scan any estimator-like fitted object
    if model is None:
        # pick first object that looks fitted: has predict and n_features_in_/classes_
        for name, obj in frame.items():
            if hasattr(obj, "predict") and hasattr(obj, "fit"):
                if any(hasattr(obj, attr) for attr in ("n_features_in_", "classes_", "feature_names_in_")):
                    model = obj
                    break
    if model is None:
        cand = [n for n, o in frame.items() if hasattr(o, "predict") and hasattr(o, "fit")]
        raise RuntimeError(f"Fitted model not found. Set FINAL_MODEL_VAR to one of: {cand}")

    # 2) choose evaluation split without NameError by probing availability
    frame = {**globals(), **locals()}
    candidates = [
        ("X_test", "y_test"),
        ("X_valid", "y_valid"),
        ("X_val", "y_val"),
        ("X_eval", "y_eval"),
        ("X_train", "y_train"),
        ("X", "y"),
    ]
    for xn, yn in candidates:
        if xn in frame and yn in frame:
            X_eval, y_eval = frame[xn], frame[yn]
            break
    else:
        raise RuntimeError(
            "No evaluation split found. Define one of: "
            "(X_test,y_test) | (X_valid,y_valid) | (X_val,y_val) | (X_train,y_train) | (X,y)."
        )

    # 3) ROC/AUC
    class_names = getattr(model, "classes_", None)
    auc_map = plot_roc_auc(model, X_eval, y_eval, class_names=class_names, title="ROC Curve (eval)")
    print("[AUC]", {k: round(v, 6) for k, v in auc_map.items()})

    # 4) SHAP (optional if shap installed)
    feature_names = list(X_eval.columns) if hasattr(X_eval, "columns") else None
    try:
        _ = analyze_feature_importance_shap(
            model,
            X_eval,
            feature_names=feature_names,
            title_prefix="SHAP (eval)"
        )
    except ImportError:
        print("SHAP not available. Install with: pip install shap")
    except Exception as e:
        print(f"SHAP analysis failed: {e}")
    
    # 9. ê²°ê³¼ ì €ì¥
    save_results(y_test, y_pred_direct, y_pred_stacked, importance_df, direct_accuracy, stacked_accuracy)
    
    # 10. ìµœì¢… ìš”ì•½
    print(f"\n" + "="*80)
    print(f"ğŸ“‹ ë…¼ë¬¸ êµ¬í˜„ ë° íƒ€ì´íƒ€ë‹‰ ì ìš© ìµœì¢… ìš”ì•½")
    print(f"="*80)
    
    print(f"\nğŸ¯ ë…¼ë¬¸ ì£¼ìš” ë‚´ìš©:")
    print(f"- ì œëª©: ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì„¤ëª…ê°€ëŠ¥í•œ AIë¥¼ í™œìš©í•œ ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ì˜ˆì¸¡")
    print(f"- ë°©ë²•ë¡ : 2ì¸µ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” (Base Classifiers + Meta Classifier)")
    print(f"- í•µì‹¬ ê¸°ë²•: One-vs-Rest ì´ì§„ ë¶„ë¥˜ + Random Forest ë©”íƒ€ ë¶„ë¥˜ê¸°")
    print(f"- ì„¤ëª…ê°€ëŠ¥ì„±: SHAPì„ í†µí•œ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
    
    print(f"\nğŸ“Š ë…¼ë¬¸ vs íƒ€ì´íƒ€ë‹‰ ì ìš© ê²°ê³¼:")
    print(f"ë…¼ë¬¸ (ëŒ€ê¸ˆì§€ê¸‰ì§€ì—° ë°ì´í„°):")
    print(f"  â€¢ ë°ì´í„°: 107,012ê°œ ì†¡ì¥, 37ê°œ íŠ¹ì„±")
    print(f"  â€¢ ì§ì ‘ ë¶„ë¥˜: 81% ì •í™•ë„")
    print(f"  â€¢ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”: 93% ì •í™•ë„ (+12%p)")
    
    print(f"\níƒ€ì´íƒ€ë‹‰ ì ìš©:")  
    print(f"  â€¢ ë°ì´í„°: 891ê°œ ìŠ¹ê°, 17ê°œ íŠ¹ì„±")
    print(f"  â€¢ ì§ì ‘ ë¶„ë¥˜: {direct_accuracy:.1%} ì •í™•ë„")
    print(f"  â€¢ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”: {stacked_accuracy:.1%} ì •í™•ë„ ({(stacked_accuracy-direct_accuracy)*100:+.1f}%p)")
    
    print(f"\nğŸ” ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:")
    print(f"1. ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ì€ ëŒ€ìš©ëŸ‰ ë³µì¡ ë°ì´í„°ì—ì„œ ì§„ê°€ë¥¼ ë°œíœ˜")
    print(f"2. ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ë‹¨ìˆœí•œ ëª¨ë¸ì´ ë•Œë¡œëŠ” ë” íš¨ê³¼ì ")
    print(f"3. ë„ë©”ì¸ íŠ¹ì„±ì„ ë°˜ì˜í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì´ í•µì‹¬")
    print(f"4. ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ìš©í•  ë•ŒëŠ” ì ì ˆí•œ ì¡°ì • í•„ìš”")
    
    print(f"\nâœ… êµ¬í˜„ ì™„ë£Œ!")

# ============================================================================
# 10. ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    main()